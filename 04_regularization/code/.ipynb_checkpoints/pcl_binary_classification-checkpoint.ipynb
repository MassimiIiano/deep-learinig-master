{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling and visualization\n",
    "\n",
    "We provide a sampler which can sample 2D labeled data points in predefined data distribution. In this assignment, we first sample the training and validation data then implement norm regularization and early stopping. To check the effects of each component, we visualize the classification result by plotting the zero-crossing line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import edf\n",
    "import copy\n",
    "\n",
    "# you can change this if you want to see other cases.\n",
    "np.random.seed(1234)\n",
    "\n",
    "class Sampler:\n",
    "\n",
    "    # hyperparameter for data distribution\n",
    "    sigma =0.1 # scale of noisy 2D offset.\n",
    "    radius = 0.3\n",
    "    center_1 = [-0.5 * radius, -radius * 0.3 ]\n",
    "    center_2 = [ 0.5 * radius, radius * 0.3 ]\n",
    "\n",
    "    def __init__(self, sigma = 0.01):\n",
    "\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def sample (self, sample_n = 10):\n",
    "\n",
    "        samples_in_uniform_dist = np.random.rand ( sample_n, 4) # 4 random variables for each sample ( binary label, angle, x,y offset )\n",
    "\n",
    "        X = np.zeros ( [sample_n , 2], dtype = np.float32 )\n",
    "        y = np.zeros ( [sample_n , 1], dtype = np.int32)\n",
    "\n",
    "        # sample the label\n",
    "        y [samples_in_uniform_dist[:, 0]>0.5] = 1\n",
    "\n",
    "        # sample the angle\n",
    "        angles = samples_in_uniform_dist[ :, 1:2 ] * np.pi\n",
    "        cos_thetas = np.cos ( angles )\n",
    "        sin_thetas = np.sin ( angles )\n",
    "\n",
    "        # sample the offset\n",
    "        offsets = norm.ppf (samples_in_uniform_dist[:,2:4])* self.sigma\n",
    "#        offsets = np.random.randn ( sample_n, 2 ) * self.sigma\n",
    "\n",
    "        X[:, :1][y == 0] = cos_thetas[y==0] * self.radius + self.center_1[0]\n",
    "        X[:, 1:2][y == 0] = sin_thetas[y==0] * self.radius + self.center_1[1]\n",
    "        X[:, :1][y == 1] = cos_thetas[y==1] * self.radius + self.center_2[0]\n",
    "        X[:, 1:2][y == 1] = -sin_thetas[y==1] * self.radius + self.center_2[1]\n",
    "\n",
    "        # add x,y offset\n",
    "        X += offsets\n",
    "\n",
    "        y = y[:,0]\n",
    "\n",
    "        return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct samplers for train and valid dataset.\n",
    "sampler_train = Sampler(0.1)\n",
    "sampler_valid = Sampler(0.1)\n",
    "\n",
    "# sample training and validation data\n",
    "X_train, y_train = sampler_train.sample(100)\n",
    "X_valid, y_valid = sampler_valid.sample(40)\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# plot point clouds\n",
    "def plot(X0, X1, title_name=\"\" ):\n",
    "    \n",
    "    plt.title(title_name)\n",
    "    plt.scatter(X0[:,0], X0[:,1], color='red', label=0)\n",
    "    plt.scatter(X1[:,0], X1[:,1], color='blue', label=1)\n",
    "    \n",
    "    plt.xlim([-0.7, 0.7])\n",
    "    plt.ylim([-0.5, 0.5])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "# draw zero-crossing line (black line)\n",
    "def plot_line (MLP):\n",
    "    p = np.linspace(-0.5, 0.5, 500)\n",
    "    x_plot, y_plot = np.meshgrid(p, p)\n",
    "    X_plot = np.stack((x_plot, y_plot), axis=2).reshape(-1, 2)\n",
    "\n",
    "    y_dummy = np.ones_like(X_plot[:,0], dtype=np.int32)\n",
    "    #\n",
    "    output = MLP.forward(X_plot)\n",
    "    output_plot = np.round(output.reshape(x_plot.shape[0], x_plot.shape[1]))\n",
    "\n",
    "    origin = 'lower'\n",
    "    plt.contour(x_plot, y_plot, output_plot, [0.5],\n",
    "                      colors=('k',),\n",
    "                      linewidths=(3,),\n",
    "                      origin=origin)\n",
    "\n",
    "# plot training data and validation data\n",
    "plot(X_train [y_train == 0], X_train [y_train == 1], title_name = 'Training dataset')\n",
    "plot(X_valid [y_valid == 0], X_valid [y_valid == 1], title_name = 'Validation dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the function that you need \n",
    "class SingleProbToProbVector(edf.CompNode):\n",
    "    def __init__(self, z):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.z = z\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = np.repeat(self.z.value, 2, axis=1)\n",
    "        self.value[:, 1] = 1 - self.value[:, 1]\n",
    "\n",
    "    def backward(self):\n",
    "        self.z.addgrad((self.grad[:, 0] - self.grad[:, 1]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm Penalty Regularization\n",
    "\n",
    "We produce the multi-layer perceptron (MLP) class which support norm penalty regularization. The framework of network is defined so your task is to implement functional part of computational nodes: `L1NormPenalty` and `L2NormPenalty`. In `MLP` class, `Construct()` is the function which builds the computational graph for training in the EDF system. `Forward()` is to compute computational nodes in the forward fashion. `Record()` is to store/load the current status of the network. You don't have to see all of them but `Construct()`.\n",
    "\n",
    "To help your understanding, I recommend you to draw the computational graph of a 2-hidden-layer MLP network including loss nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for MLP\n",
    "class MLPHyperParams:\n",
    "\n",
    "    nInputs = None\n",
    "    nHiddens = None\n",
    "    nOutputs = None\n",
    "    nLayers = None\n",
    "    PenaltyNode = None\n",
    "    enableReg = None\n",
    "    alpha = None\n",
    "\n",
    "# Multi-layer perceptron class\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, hyperParams: MLPHyperParams):\n",
    "\n",
    "        self.hyperParams = hyperParams\n",
    "\n",
    "    def construct(self):\n",
    "\n",
    "        # local variable setting\n",
    "        hyperParams = self.hyperParams\n",
    "\n",
    "        nInputs = int(hyperParams.nInputs)\n",
    "        nHiddens = int(hyperParams.nHiddens)\n",
    "        nOutputs = int(hyperParams.nOutputs)\n",
    "        nLayers = int(hyperParams.nLayers)\n",
    "        enableReg = hyperParams.enableReg\n",
    "        alpha = hyperParams.alpha\n",
    "\n",
    "        PenaltyNode = hyperParams.PenaltyNode\n",
    "\n",
    "        # clean global parameters\n",
    "        edf.clear_compgraph()\n",
    "\n",
    "        # input\n",
    "        x_node = edf.Input()\n",
    "        y_node = edf.Input()\n",
    "\n",
    "        # initialize lists\n",
    "        loss_nodes = []\n",
    "        weight_params = []\n",
    "\n",
    "        # 1st layer\n",
    "        # We generate parameters and add them in the local parameter list ( for record )\n",
    "        # we build a hidden layer as output of sigmoid of affine transformed signal x\n",
    "        param_first = edf.AffineParams(nInputs, nHiddens)\n",
    "        weight_params.append(param_first) # For record, add parameters in the local (in the instance) parameter list.\n",
    "        h = edf.Sigmoid(edf.Affine(param_first, x_node))\n",
    "\n",
    "        # Norm penalty\n",
    "        # For each parameter nodes, we create a regularization node by making the instance of PenaltyClass.\n",
    "        # PenaltyClass is a class which represents norm penalty.\n",
    "        # By changing the penalty class, we can change the type of norm penalty such as L1 and L2 norm penalty.\n",
    "        if enableReg:\n",
    "            reg_loss_node = PenaltyNode(param_first.A, alpha)\n",
    "            loss_nodes.append(reg_loss_node) # add the loss node in the loss node list \n",
    "            reg_loss_node = PenaltyNode(param_first.b, alpha)\n",
    "            loss_nodes.append(reg_loss_node) # same as previous\n",
    "\n",
    "        for i in range(nLayers - 1):\n",
    "\n",
    "            # hidden layer\n",
    "            param = edf.AffineParams(nHiddens, nHiddens)\n",
    "            weight_params.append(param) # For record, we add them in the local parameter list in the instance.\n",
    "            h = edf.Sigmoid(edf.Affine(param, h))\n",
    "\n",
    "            # Norm penalty\n",
    "            if enableReg:\n",
    "                reg_loss_node = PenaltyNode(param.A, alpha)\n",
    "                loss_nodes.append(reg_loss_node) # add the loss node in the loss node list \n",
    "                reg_loss_node = PenaltyNode(param.b, alpha)\n",
    "                loss_nodes.append(reg_loss_node) # same as previous\n",
    "\n",
    "        # the last layer\n",
    "        param_last = edf.AffineParams(nHiddens, nOutputs)\n",
    "        weight_params.append(param_last) # For record, we add them in the local parameter list in the instance.\n",
    "        output_node = edf.Sigmoid(edf.Affine(param_last, h))\n",
    "\n",
    "        # Norm penalty\n",
    "        if enableReg:\n",
    "            reg_loss_node = PenaltyNode(param_last.A, alpha)\n",
    "            loss_nodes.append(reg_loss_node) # add the loss node in the loss node list \n",
    "            reg_loss_node = PenaltyNode(param_last.b, alpha)\n",
    "            loss_nodes.append(reg_loss_node) # same as previous\n",
    "\n",
    "        # cross entropy loss\n",
    "        prob_node = SingleProbToProbVector(output_node)\n",
    "        crossloss_node = edf.CrossEntropyLoss(prob_node, y_node)\n",
    "\n",
    "        # average cross entropy loss over a minibatch\n",
    "        crossloss_avg_node = AverageMiniBatch(crossloss_node) \n",
    "        loss_nodes.append(crossloss_avg_node)\n",
    "\n",
    "        # store loss node and output node\n",
    "        self.prob_node = prob_node\n",
    "        self.output_node = output_node\n",
    "        self.total_loss_node = Add(loss_nodes) # we sum all scalar losses.\n",
    "        self.x_node = x_node\n",
    "        self.y_node = y_node\n",
    "        self.weight_params = weight_params\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "\n",
    "        if y is None:\n",
    "            y = np.ones_like ( X[:,0], dtype =np.int32 )\n",
    "        \n",
    "        # parse input\n",
    "        self.x_node.value = X\n",
    "        self.y_node.value = y\n",
    "\n",
    "        # run\n",
    "        edf.Forward()\n",
    "\n",
    "        # get output\n",
    "        output = np.round(self.output_node.value)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def load (self):\n",
    "        \n",
    "        # local variable setting\n",
    "        hyperParams = self.hyperParams\n",
    "\n",
    "        nInputs = int(hyperParams.nInputs)\n",
    "        nHiddens = int(hyperParams.nHiddens)\n",
    "        nOutputs = int(hyperParams.nOutputs)\n",
    "        nLayers = int(hyperParams.nLayers)\n",
    "        enableReg = hyperParams.enableReg\n",
    "        alpha = hyperParams.alpha\n",
    "        PenaltyNode = hyperParams.PenaltyNode\n",
    "        \n",
    "        # load overall parameters from the recorded instance\n",
    "        self.weight_params = self.rep.weight_params\n",
    "        \n",
    "        # clean global parameters\n",
    "        edf.clear_compgraph()\n",
    "\n",
    "        # input\n",
    "        x_node = edf.Input()\n",
    "        y_node = edf.Input()\n",
    "\n",
    "        loss_nodes = []\n",
    "        # 1st layer\n",
    "\n",
    "        param_first = self.weight_params[0]\n",
    "        h = edf.Sigmoid(edf.Affine(param_first, x_node))\n",
    "\n",
    "        # reg\n",
    "        if enableReg:\n",
    "            reg_loss_node = PenaltyNode(param_first.A, alpha)\n",
    "            loss_nodes.append(reg_loss_node)\n",
    "            reg_loss_node = PenaltyNode(param_first.b, alpha)\n",
    "            loss_nodes.append(reg_loss_node)\n",
    "\n",
    "        for i in range(nLayers - 1):\n",
    "\n",
    "            # hidden layer\n",
    "            param = self.weight_params[i+1]\n",
    "            h = edf.Sigmoid(edf.Affine(param, h))\n",
    "\n",
    "            # reg\n",
    "            if enableReg:\n",
    "                reg_loss_node = PenaltyNode(param.A, alpha)\n",
    "                loss_nodes.append(reg_loss_node)\n",
    "                reg_loss_node = PenaltyNode(param.b, alpha)\n",
    "                loss_nodes.append(reg_loss_node)\n",
    "\n",
    "        # the last layer\n",
    "        param_last = self.weight_params[-1]\n",
    "        output_node = edf.Sigmoid(edf.Affine(param_last, h))\n",
    "\n",
    "        # reg loss\n",
    "        if enableReg:\n",
    "            reg_loss_node = PenaltyNode(param_last.A, alpha)\n",
    "            loss_nodes.append(reg_loss_node)\n",
    "            reg_loss_node = PenaltyNode(param_last.b, alpha)\n",
    "            loss_nodes.append(reg_loss_node)\n",
    "\n",
    "        # cross entropy loss\n",
    "        prob_node = SingleProbToProbVector(output_node)\n",
    "        crossloss_node = edf.CrossEntropyLoss(prob_node, y_node)\n",
    "        crossloss_avg_node = AverageMiniBatch(crossloss_node)\n",
    "        loss_nodes.append(crossloss_avg_node)\n",
    "\n",
    "        # store loss node and output node\n",
    "        self.prob_node = prob_node\n",
    "        self.output_node = output_node\n",
    "        self.total_loss_node = Add(loss_nodes)\n",
    "        self.x_node = x_node\n",
    "        self.y_node = y_node\n",
    "\n",
    "    def record(self):\n",
    "        self.rep = copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the total loss as the single scalar value, we add new computational nodes: `Add` and `AverageMiniBatch`. We first compute an average of the cross-entropy losses over minibatches then we sum all losses as the total loss: regularization loss for each parameter and the average of the cross-entropy losses over minibatches. In this way, we donot have to modify `edf.py` at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(edf.CompNode):\n",
    "\n",
    "    def __init__(self, x_list):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x_list = x_list\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        value = np.zeros_like(self.x_list[0].value, self.x_list[0].value.dtype)\n",
    "\n",
    "        for x in self.x_list:\n",
    "            value += x.value\n",
    "\n",
    "        self.value = value\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        for x in self.x_list:\n",
    "            x.addgrad(np.ones_like(x.value, x.value.dtype)  * self.grad[0])\n",
    "\n",
    "class AverageMiniBatch(edf.CompNode):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        \n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        value = np.average(self.x.value)\n",
    "        self.value = np.resize(value, (1, 1))\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        self.x.addgrad(np.ones_like(self.x.value, self.x.value.dtype)/self.x.value.size * self.grad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and visualization of the network trained without norm regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_steps, x, y, x_node, y_node, prob_node, loss_node):\n",
    "\n",
    "    x_node.value = x\n",
    "    y_node.value = y\n",
    "    dataset_size = x.shape[0]\n",
    "    \n",
    "    for iteration in range(1, num_steps + 1):\n",
    "        \n",
    "        edf.Forward()\n",
    "        total_err = np.sum(np.not_equal(np.argmax(prob_node.value, axis=1), y_node.value))\n",
    "\n",
    "        edf.Backward(loss_node)\n",
    "        edf.UpdateParameters()\n",
    "        \n",
    "        if iteration in [100, 200, 500, 1000, 5000] or iteration % 10000 == 0:\n",
    "            print('iter: {}, error: {:6f}'.format(iteration, total_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get familiar with the `MLP` class. By passing proper hyperparameters, we can construct the `MLP` instance which can construct the MLP nework in the `edf` system on our purpose. In the following code by disenabling norm penalty regularization, our `MLP` instance construct the MLP network without norm penalty regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100, error: 28.000000\n",
      "iter: 200, error: 23.000000\n",
      "iter: 500, error: 22.000000\n",
      "iter: 1000, error: 21.000000\n",
      "iter: 5000, error: 6.000000\n",
      "iter: 10000, error: 6.000000\n",
      "iter: 20000, error: 5.000000\n"
     ]
    }
   ],
   "source": [
    "hyperParams = MLPHyperParams()\n",
    "\n",
    "hyperParams.nInputs = 2\n",
    "hyperParams.nHiddens = 16\n",
    "hyperParams.nOutputs = 1 \n",
    "hyperParams.nLayers= 2\n",
    "\n",
    "## You have to see these two lines. ##\n",
    "hyperParams.enableReg = False ## THESE\n",
    "hyperParams.PenaltyNode = None            ## LINES\n",
    "\n",
    "hyperParams.alpha = 0.000\n",
    "\n",
    "MLP_no_reg = MLP ( hyperParams )\n",
    "MLP_no_reg.construct()\n",
    "\n",
    "#training\n",
    "num_steps = 20000\n",
    "learning_rate = 1\n",
    "edf.learning_rate = learning_rate\n",
    "\n",
    "train(num_steps, X, y, MLP_no_reg.x_node, MLP_no_reg.y_node, MLP_no_reg.prob_node, MLP_no_reg.total_loss_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visualize the zero-crossing line with the training dataset and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line (MLP_no_reg)\n",
    "plot(X_train[y_train==0], X_train[y_train==1], title_name = 'training')\n",
    "\n",
    "plot_line (MLP_no_reg)\n",
    "plot(X_valid[y_valid==0], X_valid[y_valid==1], title_name = 'validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and visualization of the network trained with L1 regularization\n",
    "\n",
    "We can build a MLP network with norm penalty terms by passing a norm penalty class as a parameter. Let us construct a MLP network with `L1` norm regularization, train the network then see the classification result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to fill part of `L1NormPenalty` class which represents L1 norm penalty. In `forward` function, you compute a scalar value of the L1-norm penalty. In `backward`, you have to compute the gradient of L1-norm penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class L1NormPenalty(edf.CompNode):\n",
    "\n",
    "    def __init__(self, x, alpha):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        ###############################################\n",
    "        # TO-DO : Implement a forward part.\n",
    "        # Compute the L1-norm penalty of the parameters\n",
    "        # You should store a scalar value in variable 'value'.\n",
    "\n",
    "        # ex:\n",
    "        # value = \n",
    "        ###############################################\n",
    "        \n",
    "        self.value = np.resize(value, (1, 1))\n",
    "        \n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "        ###############################################\n",
    "        # TO-DO : Implement a backward part.\n",
    "        # Compute the gradient of L1 norm penalty and do backward propagation\n",
    "        ###############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the `MLP` object with L1 norm penalty. By setting the hyperparameter of the penalty node class properly, our `MLP` instance constructs the computational graph in the `edf` system automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = MLPHyperParams()\n",
    "\n",
    "hyperParams.nInputs = 2\n",
    "hyperParams.nHiddens = 16\n",
    "hyperParams.nOutputs = 1 \n",
    "hyperParams.nLayers= 3\n",
    "hyperParams.enableReg = True\n",
    "\n",
    "###############################################\n",
    "# TO-DO : assign the proper PenaltyNode class to the MLP hyperparameter.\n",
    "# Hint: hyperParams.PenaltyNode = \n",
    "###############################################\n",
    "\n",
    "hyperParams.PenaltyNode = \n",
    "hyperParams.alpha = 0.001\n",
    "\n",
    "MLP_with_L1 = MLP ( hyperParams )\n",
    "MLP_with_L1.construct()\n",
    "\n",
    "#training\n",
    "num_steps = 20000\n",
    "learning_rate = 1\n",
    "edf.learning_rate = learning_rate\n",
    "\n",
    "train(num_steps, X, y, MLP_with_L1.x_node, MLP_with_L1.y_node, MLP_with_L1.prob_node, MLP_with_L1.total_loss_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visualize the zero-crossing line with the training dataset and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line (MLP_with_L1)\n",
    "plot(X_train[y_train==0], X_train[y_train==1], title_name = 'training')\n",
    "\n",
    "plot_line (MLP_with_L1)\n",
    "plot(X_valid[y_valid==0], X_valid[y_valid==1], title_name = 'validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and visualization of the network trained with L2 regularization\n",
    "\n",
    "Your task is to fill part of `L2NormPenalty` class which represents L2 norm penalty. In `forward` function, you compute a scalar value of the L2 norm penalty. In `backward`, you have to compute the gradient of L2 norm penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2NormPenalty(edf.CompNode):\n",
    "\n",
    "    def __init__(self, x, alpha):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        ###############################################\n",
    "        # TO-DO : Implement a forward part.\n",
    "        # Compute the L2-norm penalty of the parameters\n",
    "        # You should store a scalar value in variable 'value'.\n",
    "        # ex:\n",
    "        # value = \n",
    "        ###############################################\n",
    "\n",
    "        \n",
    "        self.value = np.resize(value, (1, 1))\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        ###############################################\n",
    "        # TO-DO : Implement a backward part.\n",
    "        # Compute the gradient of L2 norm penalty and do backward propagation\n",
    "        ###############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous, we generate the `MLP` object with L2 norm penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = MLPHyperParams()\n",
    "\n",
    "hyperParams.nInputs = 2\n",
    "hyperParams.nHiddens = 16\n",
    "hyperParams.nOutputs = 1 \n",
    "hyperParams.nLayers= 2\n",
    "hyperParams.enableReg = True\n",
    "\n",
    "###############################################\n",
    "# TO-DO : assign the L2 PenaltyNode class to the MLP hyperparameter.\n",
    "# Hint: hyperParams.PenaltyNode = \n",
    "###############################################\n",
    "\n",
    "hyperParams.PenaltyNode = \n",
    "hyperParams.alpha = 0.001\n",
    "\n",
    "MLP_with_L2 = MLP ( hyperParams )\n",
    "MLP_with_L2.construct()\n",
    "\n",
    "#training\n",
    "num_steps = 20000\n",
    "learning_rate = 1\n",
    "edf.learning_rate = learning_rate\n",
    "\n",
    "train(num_steps, X, y, MLP_with_L2.x_node, MLP_with_L2.y_node, MLP_with_L2.prob_node, MLP_with_L2.total_loss_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visualize the zero-crossing line with the training dataset and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line (MLP_with_L2)\n",
    "plot(X_train[y_train==0], X_train[y_train==1], title_name = 'training')\n",
    "\n",
    "plot_line (MLP_with_L2)\n",
    "plot(X_valid[y_valid==0], X_valid[y_valid==1], title_name = 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Implement early stopping. By using `MLP.record()`, you can record the current state in `MLP.rep` variable. Your task is to call `MLP.record()` when a minimum of a validation error decreases. If it does not decrease for the certain number of iterations, we stop training. To this end, we give two arguments: `validation_step` and `max_passed_step`. We compute the validation error and do early stopping test for each `validation_step` step. If there is no renew of the validation minimum for `max_passed_step`, we finalize the training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_early_stopping(num_steps, x_train, y_train, x_valid, y_valid, x_node, y_node, prob_node, loss_node, MLP, validation_step = 100, max_passed_step = 5000):\n",
    "\n",
    "    dataset_size = x_train.shape[0]\n",
    "    \n",
    "    validation_error = 10\n",
    "    \n",
    "    ###############################################\n",
    "    # How to find a minimum value in the list.\n",
    "    #\n",
    "    # first initialize the minimum as the big number. (or set the first element value. )\n",
    "    # Then, renew the minimum during iteration.\n",
    "    #\n",
    "    # ex)\n",
    "    # min_value = 100000\n",
    "    # for i in range(len(sequence)):\n",
    "    #        if sequence[i] < min_value:\n",
    "    #            min_value = sequence[i]\n",
    "    ###############################################\n",
    "    \n",
    "    ###############################################\n",
    "    # TO-DO : Initialize the current minimum variable\n",
    "    #    \n",
    "    ###############################################\n",
    "    \n",
    "    for iteration in range(1, num_steps + 1):\n",
    "       \n",
    "        if iteration % validation_step == 0:\n",
    "            \n",
    "            x_node.value = x_valid\n",
    "            y_node.value = y_valid\n",
    "\n",
    "            edf.Forward()\n",
    "\n",
    "            current_err_valid = np.sum(np.not_equal(np.argmax(prob_node.value, axis=1), y_node.value))\n",
    "\n",
    "            print('iter: {}, error: {:6f} {:6f}'.format(iteration, current_err_train, current_err_valid),end='')\n",
    "\n",
    "            ###############################################\n",
    "            # TO-DO : \n",
    "            # when the current validation error is minimum over all iterations, record the current parameter status.\n",
    "            # If the number of passed steps is over than the certain number of steps, finalize the training session.\n",
    "\n",
    "            # Hint : you can use MLP.record() to store the status\n",
    "            ###############################################\n",
    "            \n",
    "            print('')\n",
    "\n",
    "    \n",
    "        x_node.value = x_train\n",
    "        y_node.value = y_train\n",
    "        \n",
    "        edf.Forward()\n",
    "                \n",
    "        current_err_train = np.sum(np.not_equal(np.argmax(prob_node.value, axis=1), y_node.value))\n",
    "\n",
    "        edf.Backward(loss_node)\n",
    "        edf.UpdateParameters()\n",
    "        \n",
    "        # print(loss_node.value.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us test how our early stopping works. To test it, we build a MLP network without norm penalty. Then, we train the network in the early-stopping fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = MLPHyperParams()\n",
    "\n",
    "hyperParams.nInputs = 2\n",
    "hyperParams.nHiddens = 16\n",
    "hyperParams.nOutputs = 1 \n",
    "hyperParams.nLayers= 2\n",
    "\n",
    "## You have to see these two lines. ##\n",
    "hyperParams.enableReg = False ## THESE\n",
    "hyperParams.PenaltyNode = None            ## LINES !!\n",
    "\n",
    "hyperParams.alpha = 0.000\n",
    "\n",
    "MLP_no_reg = MLP ( hyperParams )\n",
    "MLP_no_reg.construct()\n",
    "\n",
    "#training\n",
    "num_steps = 20000\n",
    "learning_rate = 1\n",
    "edf.learning_rate = learning_rate\n",
    "\n",
    "train_early_stopping(num_steps, X_train, y_train, X_valid, y_valid, MLP_no_reg.x_node, MLP_no_reg.y_node, MLP_no_reg.prob_node, MLP_no_reg.total_loss_node, MLP_no_reg, validation_step = 100, max_passed_step =5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visualize the zero-crossing line with the training dataset and the validation dataset. `MLP.load` loads all recorded parameters and constructs the MLP network in the `edf` system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_no_reg.load()\n",
    "\n",
    "plot_line (MLP_no_reg)\n",
    "plot(X_train[y_train==0], X_train[y_train==1], title_name = 'training')\n",
    "\n",
    "plot_line (MLP_no_reg)\n",
    "plot(X_valid[y_valid==0], X_valid[y_valid==1], title_name = 'validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
