{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "First we need to load the MNIST dataset from disk. We will do 10-class classification for digit 0, 1, .., 9 from the MNIST dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (60000, 28, 28)\n",
      "Train labels shape:  (60000,)\n",
      "Test data shape:  (10000, 28, 28)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import edf\n",
    "import mnist_loader\n",
    "import copy\n",
    "\n",
    "train_images, train_labels = mnist_loader.load_mnist(section = 'training', path = 'MNIST')\n",
    "test_images, test_labels = mnist_loader.load_mnist(section = 'testing', path = 'MNIST')\n",
    "\n",
    "# quickly check the shape of data\n",
    "print('Train data shape: ', train_images.shape)\n",
    "print('Train labels shape: ', train_labels.shape)\n",
    "print('Test data shape: ', test_images.shape)\n",
    "print('Test labels shape: ', test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A multi-layer perceptron (MLP) network with dropout\n",
    "\n",
    "We produce the multi-layer perceptron (MLP) class. In this assignment, we implement dropout by adding `dropout` layers between hidden layers. The framework of network is defined below so your task is to implement functional part of `dropout` computational nodes. To help your understanding, I recommend you to draw the computational graph of a 2-layer MLP network including loss nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "class MLPParams:\n",
    "\n",
    "    nInputs = None\n",
    "    nHiddens = None\n",
    "    nOutputs = None\n",
    "    nLayers = None\n",
    "    ActivationClass = None\n",
    "    enableReg = None\n",
    "    RegClass = None\n",
    "    alpha = None\n",
    "    dropOutProb = None\n",
    "    \n",
    "# MLP\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, params: MLPParams ):\n",
    "        self.params = params\n",
    "\n",
    "    def construct(self):\n",
    "\n",
    "        params = self.params\n",
    "\n",
    "        nInputs = int(params.nInputs)\n",
    "        nHiddens = int(params.nHiddens)\n",
    "        nOutputs = int(params.nOutputs)\n",
    "        nLayers = int(params.nLayers)\n",
    "        ActivationClass = params.ActivationClass\n",
    "        RegClass = params.RegClass\n",
    "        enableReg = params.enableReg\n",
    "        alpha = params.alpha\n",
    "        dropOutProb = params.dropOutProb\n",
    "\n",
    "        if RegClass is None:\n",
    "            enableReg = False\n",
    "        \n",
    "        # clean global parameters\n",
    "        edf.clear_compgraph()\n",
    "\n",
    "        # input\n",
    "        x_node = edf.Input()\n",
    "        y_node = edf.Input()\n",
    "        loss_nodes = []\n",
    "        weight_params = []\n",
    "        \n",
    "        # 1st layer\n",
    "        param_first = edf.AffineParams(nInputs, nHiddens)\n",
    "        weight_params.append( param_first )\n",
    "        h = DropOut ( ActivationClass (edf.Affine( param_first, x_node )), dropOutProb)\n",
    "\n",
    "        # reg\n",
    "        if enableReg:\n",
    "            reg_loss_node = self.RegClass(param_first.A, alpha)\n",
    "            loss_nodes.append(reg_loss_node)\n",
    "            reg_loss_node = self.RegClass(param_first.b, alpha)\n",
    "            loss_nodes.append(reg_loss_node)\n",
    "\n",
    "        for i in range(nLayers - 1):\n",
    "\n",
    "            # hidden layer\n",
    "            param = edf.AffineParams(nHiddens, nHiddens)\n",
    "            weight_params.append(param)\n",
    "            h = DropOut ( ActivationClass ( edf.Affine(param, h) ), dropOutProb)\n",
    "\n",
    "            if enableReg:\n",
    "                reg_loss_node = self.RegClass(param.A, alpha)\n",
    "                loss_nodes.append(reg_loss_node)\n",
    "                reg_loss_node = self.RegClass(param.b, alpha)\n",
    "                loss_nodes.append(reg_loss_node)\n",
    "\n",
    "        # the last layer\n",
    "        param_last = edf.AffineParams(nHiddens, nOutputs)\n",
    "        weight_params.append( param_last )\n",
    "        #prob_node = DropOut ( edf.Softmax (ActivationClass ( edf.Affine(param, h) )), dropOutProb)\n",
    "        prob_node = edf.Softmax(edf.Affine(param_last, h))\n",
    "\n",
    "        if enableReg:\n",
    "            reg_loss_node = self.RegClass(param_last.A, alpha)\n",
    "            loss_nodes.append(reg_loss_node)\n",
    "            reg_loss_node = self.RegClass(param_last.b, alpha)\n",
    "            loss_nodes.append(reg_loss_node)\n",
    "        \n",
    "        # loss\n",
    "        crossloss_node = edf.CrossEntropyLoss(prob_node, y_node)\n",
    "        sum_node = AverageMiniBatch(crossloss_node)\n",
    "        loss_nodes.append(sum_node)\n",
    "\n",
    "        # probability\n",
    "        self.x_node = x_node\n",
    "        self.y_node = y_node\n",
    "        self.loss_node = Add(loss_nodes)\n",
    "        self.prob_node = prob_node\n",
    "        self.output_node = prob_node\n",
    "        self.weight_params = weight_params\n",
    "\n",
    "    def forward(self, X,y = None):\n",
    "        \n",
    "        if y is None:\n",
    "            y = np.ones_like ( X[:,0],dtype =np.int32)\n",
    "\n",
    "        # parse input\n",
    "        self.x_node.value = X\n",
    "        self.y_node.value = y\n",
    "\n",
    "        # run\n",
    "        edf.Forward()\n",
    "\n",
    "        # get output\n",
    "        output = np.round(self.output_node.value)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def load (self):\n",
    "        print(\"TBA\")\n",
    "\n",
    "    def record (self):\n",
    "        self.rep = copy.deepcopy(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide a `ReLU` activation node and computational nodes for our MLP network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add\n",
    "class Add(edf.CompNode):\n",
    "\n",
    "    def __init__(self, x_list):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x_list = x_list\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        value = np.zeros_like(self.x_list[0].value, self.x_list[0].value.dtype)\n",
    "\n",
    "        for x in self.x_list:\n",
    "            value += x.value\n",
    "\n",
    "        self.value = value\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        for x in self.x_list:\n",
    "            x.addgrad(np.ones_like(x.value, x.value.dtype)* self.grad[0]) \n",
    "\n",
    "# Average loss over a minibach\n",
    "class AverageMiniBatch(edf.CompNode):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        value = np.average(self.x.value)\n",
    "        self.value = np.resize(value, (1, 1))\n",
    "\n",
    "    def backward(self):\n",
    "        self.x.addgrad(np.ones_like(self.x.value, self.x.value.dtype)/self.x.value.size* self.grad[0]) \n",
    "\n",
    "# ReLU\n",
    "class ReLU(edf.CompNode):\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = np.maximum(0, self.x.value)\n",
    "        # pass\n",
    "\n",
    "    def backward(self):\n",
    "        # pass\n",
    "        v = np.ones_like(self.x.value)\n",
    "        v[self.x.value <= 0.0] = 0.0\n",
    "        self.x.addgrad(self.grad * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Implement backward and forward parts of a `Dropout` node. In the forward function, we first generate the random mask according to the dropout probability. Then, we filter out the input signal, using the mask. In the backward function, we do backpropagation only on the valid part. Note that dropout should be performed during training. To this end, we use `edf.enable_dropout` to specify whether dropout should be performed or not. If `edf.enable_dropout` is false, this node give input as output without any operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class DropOut ( edf.CompNode ):\n",
    "\n",
    "    def __init__(self, x, dropout_prob = 0.5):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        mask = np.ones_like(self.x)\n",
    "        value = self.x.value\n",
    "        if edf.enable_dropout:\n",
    "\n",
    "            #########################################################################\n",
    "            # TO-DO: implement the forward part of a dropout computation node.\n",
    "            # when dropout is activated,\n",
    "            # you first generate the random mask which has the same size with input x.\n",
    "            # then you filter out signal x according to the generated mask.\n",
    "            #########################################################################\n",
    "\n",
    "\n",
    "        self.mask = np.array( mask , dtype = edf.DT)\n",
    "        self.value = value\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        #########################################################################\n",
    "        # TO-DO: implement the backward part of a dropout computation node.\n",
    "        # You compute the gradient while considering the mask we used for forward.\n",
    "        # then you filter out signal x according to the generated mask.\n",
    "        #########################################################################\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide functions for training. By passing `optimizer` and `transformer` objects into `train_and_test` function, we can train the network with various optimization methods and data augmentation. About `transformer`, we will talk later. Here we provide the basic optimizer, which update the parameters based on pure gradients and the learning rate. Note that the learning rate is multiplied with the gradient and the multiplication is added to the parameter in `p.UpdateParameters()`. Please see the `edf.py` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic gradient descent method\n",
    "class GDOptimizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"\")\n",
    "\n",
    "    def initialize (self):\n",
    "        print(\"\")\n",
    "           \n",
    "    def updateParameters (self):\n",
    "\n",
    "        for p in edf.Parameters:\n",
    "            p.UpdateParameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(batch_size, data, labels, x_node, y_node, prob_node, loss_node = None, optimizer=GDOptimizer(), transformer = None):\n",
    "\n",
    "    num_samples = len(data)\n",
    "    total_err = 0.0\n",
    "    num_batches = num_samples//batch_size\n",
    "\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start, end = i*batch_size, (i+1)*batch_size\n",
    "\n",
    "        # load supervised training data\n",
    "        if transformer is None:\n",
    "            x_node.value = data[start:end].reshape(end - start, -1)\n",
    "        else:\n",
    "            x_node.value = transformer.transform ( data[start:end] ).reshape(end - start, -1)\n",
    "  \n",
    "        y_node.value = labels[start:end]\n",
    "\n",
    "        #  forward\n",
    "        edf.Forward()\n",
    "\n",
    "        # compute error\n",
    "        total_err += np.sum(np.not_equal(np.argmax(prob_node.value, axis=1), y_node.value))\n",
    "\n",
    "\n",
    "        # step\n",
    "        if loss_node:\n",
    "\n",
    "            # backpropagation\n",
    "            edf.Backward(loss_node)\n",
    "            optimizer.updateParameters()\n",
    "\n",
    "        if i>0 and i%400 == 0:\n",
    "            print (\"\\t Batch {}/{}\".format(i, num_batches))\n",
    "\n",
    "    return 100*total_err/num_samples\n",
    "\n",
    "def train_and_test(num_epochs, batch_size, x_node, y_node, prob_node, loss_node, optimizer=GDOptimizer(), transformer=None, enable_dropout = False):\n",
    "\n",
    "    train_err_log = []\n",
    "    test_err_log = []\n",
    "    \n",
    "    # initialize optimizer\n",
    "    optimizer.initialize()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"Epoch: {}/{} (learning rate: {})\".format(epoch+1, num_epochs, edf.learning_rate))\n",
    "\n",
    "        edf.enable_dropout = enable_dropout\n",
    "        train_err = run_epoch(batch_size, train_images, train_labels, x_node, y_node, prob_node, loss_node, optimizer, transformer)\n",
    "        train_err_log.append(train_err)\n",
    "        print (\"\\t Training Error {:.2f} %\".format(train_err))\n",
    "\n",
    "        edf.enable_dropout = False\n",
    "        test_err = run_epoch(len(test_images), test_images, test_labels, x_node, y_node, prob_node, None, optimizer, None)\n",
    "        test_err_log.append(test_err)\n",
    "        print (\"\\t Test Error {:.2f} %\".format(test_err))\n",
    "        \n",
    "        # print out the error late per label.\n",
    "        for i in range(10):\n",
    "            err_n_per_i = np.sum(np.not_equal(np.argmax(prob_node.value[y_node.value==i], axis=1), y_node.value[y_node.value==i]))\n",
    "            n_per_i = np.sum(y_node.value==i)\n",
    "            print( i,\" : \", err_n_per_i,'/', n_per_i,'  ', '%.2f' % (err_n_per_i / n_per_i))\n",
    "        \n",
    "\n",
    "    return train_err_log, test_err_log\n",
    "\n",
    "def plot(train_err_log, test_err_log):\n",
    "\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"error (%)\")\n",
    "    plt.plot(np.arange(len(test_err_log)), test_err_log, color='red')\n",
    "    plt.plot(np.arange(len(train_err_log)), train_err_log, color='blue')\n",
    "    plt.legend(['test error', 'train error'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a MLP network and train the network without data augmentation. You can activate norm regularization by adding regularization class in this jupyter notebook and changing parameter settings. After constructing the MLP network, train the network and show the error graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = MLPParams()\n",
    "param.nInputs = 784 # 784-dimension\n",
    "param.nOutputs = 10 # Output dimension\n",
    "param.nLayers = 3\n",
    "param.nHiddens = 64 # Number of neurons in the hidden layer\n",
    "param.ActivationClass = ReLU\n",
    "param.RegClass = None\n",
    "param.alpha = 0.001\n",
    "param.dropOutProb = 0.5\n",
    "mlp_ReLU = MLP(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_ReLU.construct()\n",
    "\n",
    "\n",
    "#Train\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.1\n",
    "\n",
    "###############################################################\n",
    "# TO-DO: call the train_and_test function activating dropout mode.\n",
    "# Hint : train_err_log, test_err_log = train_and_test(num_epochs, batch_size, mlp_ReLU.x_node, mlp_ReLU.y_node, mlp_ReLU.prob_node, mlp_ReLU.loss_node, ? )\n",
    "###############################################################\n",
    "\n",
    "\n",
    "plot(train_err_log, test_err_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network without dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_ReLU.construct()\n",
    "\n",
    "#Train\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.1\n",
    "\n",
    "###############################################################\n",
    "# TO-DO: call the train_and_test function activating dropout mode.\n",
    "# Hint : train_err_log, test_err_log = train_and_test(num_epochs, batch_size, mlp_ReLU.x_node, mlp_ReLU.y_node, mlp_ReLU.prob_node, mlp_ReLU.loss_node, ? )\n",
    "###############################################################\n",
    "\n",
    "\n",
    "plot(train_err_log, test_err_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation: vertical and horizontal flip\n",
    "\n",
    "In this assignment, instead of increasing the variety of valid data pools, we pollute the training data set by flipping images. Let us check how the accuracy changes. To augment the dataset, we now implement the augmentation  class. We transform the input data before the network is trained. Your task is to fill in `RandomFlipAug` class. You can refer to other augmentation class to understand how the augmentation class works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "# Gaussian noise\n",
    "class RandomNoiseAug:\n",
    "\n",
    "    amplitude_max = 10\n",
    "\n",
    "    def __init__(self, amplitude_max):\n",
    "        self.amplitude_max = amplitude_max\n",
    "\n",
    "    def transform (self, x):\n",
    "\n",
    "        image_n = x.shape[0]\n",
    "        x_new = copy.deepcopy(x)\n",
    "\n",
    "        #\n",
    "        noise_max = np.random.rand ( image_n, 1, 1 ) * self.amplitude_max\n",
    "        gaussian_noise = np.random.randn ( x.shape[0], x.shape[1],x.shape[2] )\n",
    "        gaussian_noise = noise_max * gaussian_noise\n",
    "\n",
    "        x_new = gaussian_noise + x_new\n",
    "        x_new [x_new<0] = 0\n",
    "        x_new [x_new>1] = 1\n",
    "\n",
    "        return x_new\n",
    "\n",
    "# Translation\n",
    "class RandomTranslationAug:\n",
    "\n",
    "    width = None\n",
    "\n",
    "    def __init__(self, width):\n",
    "        self.width = width\n",
    "\n",
    "    def transform ( self, x ):\n",
    "\n",
    "        image_n = x.shape[0]\n",
    "        x_new = copy.deepcopy(x)\n",
    "\n",
    "        trans_cols = np.array(np.round(np.random.rand ( image_n ) * self.width - 0.4999), dtype = np.int32 )\n",
    "        trans_rows = np.array(np.round(np.random.rand ( image_n ) * self.width - 0.4999), dtype = np.int32 )\n",
    "\n",
    "        trans_cols -= int(self.width/2)\n",
    "        trans_rows -= int(self.width/2)\n",
    "\n",
    "\n",
    "        # translate for each imgae\n",
    "        for i in range(0,image_n):\n",
    "\n",
    "            # wrapping\n",
    "            x_temp = np.tile(x[i,:,:], [3, 3])\n",
    "\n",
    "            trans_col = int( trans_cols[i])\n",
    "            trans_row = int(trans_rows[i])\n",
    "\n",
    "            x_new[i,:,:]=x_temp[ x.shape[1]+trans_row:x.shape[1]*2+trans_row, x.shape[2] + trans_col : x.shape[2] * 2 + trans_col ]\n",
    "\n",
    "        return x_new\n",
    "\n",
    "# Flip\n",
    "class RandomFlipAug:\n",
    "\n",
    "    flip_prob_vertical = 0.5\n",
    "    flip_prob_horizontal = 0.5\n",
    "\n",
    "    def __init__(self, prob_vertical, prob_horizontal ):\n",
    "        self.flip_prob_vertical = prob_vertical\n",
    "        self.flip_prob_horizontal = prob_horizontal\n",
    "\n",
    "    def transform(self, x):\n",
    "\n",
    "        image_n = x.shape[0]\n",
    "        x_new = copy.deepcopy(x)\n",
    "       \n",
    "        ##############################################################\n",
    "        # TO-DO: first generate random samples to decide flipping or not.\n",
    "        # If the random samples are below than flip_prob_vertical we do vertical flip. \n",
    "        # If the random samples are below than flip_prob_horizontal we do horizontal flip.\n",
    "        # You should generate two random sample per each image.   \n",
    "        ##############################################################\n",
    "\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance after we flip the training dataset horizontally. How the performance is changed depending on the label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = MLPParams()\n",
    "param.nInputs = 784 # 784-dimension\n",
    "param.nOutputs = 10 # Output dimension\n",
    "param.nLayers = 2\n",
    "param.nHiddens = 64 # Number of neurons in the hidden layer\n",
    "param.ActivationClass = ReLU\n",
    "param.RegClass = None\n",
    "param.alpha = 0.001\n",
    "param.dropOutProb = 0.5\n",
    "\n",
    "mlp_ReLU = MLP(param)\n",
    "\n",
    "mlp_ReLU.construct()\n",
    "\n",
    "#Train\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.1\n",
    "\n",
    "# Horizontal flip\n",
    "#########################################\n",
    "# TO-DO : generate the RandomFlipAug instance which always flip images horizontally.\n",
    "# Hint : AugObject =  \n",
    "#########################################\n",
    "\n",
    "\n",
    "train_err_log, test_err_log = train_and_test(num_epochs, batch_size, mlp_ReLU.x_node, mlp_ReLU.y_node, mlp_ReLU.prob_node, mlp_ReLU.loss_node, transformer = AugObject, enable_dropout=False)\n",
    "\n",
    "plot(train_err_log, test_err_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance after we flip the training dataset vertically. How the performance is changed depending on the label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_ReLU.construct()\n",
    "\n",
    "#Train\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.1\n",
    "\n",
    "# Vertical flip\n",
    "#########################################\n",
    "# TO-DO : generate the RandomFlipAug instance which always flip images vertically.\n",
    "# Hint : AugObject =  \n",
    "#########################################\n",
    "\n",
    "\n",
    "train_err_log, test_err_log = train_and_test(num_epochs, batch_size, mlp_ReLU.x_node, mlp_ReLU.y_node, mlp_ReLU.prob_node, mlp_ReLU.loss_node, transformer = AugObject, enable_dropout = False)\n",
    "\n",
    "plot(train_err_log, test_err_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
